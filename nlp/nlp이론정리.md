# 개요

### 텍스트 마이닝

` 언어학, 통계학, 기계학습 등을 기반 자연어 처리기술을 활용하여  텍스트데이터를 정형화 하고, 특징을 추출하기 위한 기술과 추출된 특징으로부터 의미있는 정보를 발견할 수 있도록 하는 기술`



- 예시 : 
  - 뉴스레터 메일링/ SNS 자동 포스팅
  - 경쟁사 분석
  - 실시간 리뷰 모니터링
  - 고객분석



자연어 처리 텍스트 분석 절차

1. 데이터 수집 단계
2. 텍스트 전처리
3. 텍스트 분석
4. 시각화



---

### 텍스트 전처리

#### 1.  토큰화

문장토큰화, 단어 토큰화, 형태소 분석



#### 2. 품사 부착(POS Tagging)

개체명 인식(NER) : 사람, 조직, 지역, 날짜, 숫자 등의 개체 유형을 식별

- 청킹(Chunking) : 정보를 의미 있는 단위로 묶어주는 기술 (ex:금융 통화 위원회)



#### 3. 원형복원(Stemming, Lemmatzing)

- 어간 추출(stemming) - 활용시 변하지 않는 부분(ex:먹고 먹지 먹는-> 먹)
- 표제어 추출(lemmatization) - 사전에 등재된 단어(ex: 나무들 ->나무)

#### 4. 불용어 처리

- 의미가 없는 단어 토큰을 제거하는 작업

  ```python
  from nltk.corpus import stopwords
  ```

  목적에 맞게 설정



### 텍스트 분석

1. 주제어 찾기(Topic Modeling)
2. 문서 요약(Text Summarize) : 문서 내에서 주요 문장을 찾아 요약
3. 문서 분류(Category Classification) : 단어, 문장을 분석하여 분서분류
4. 감성 분석(Sentiment Analysis) : 문서내 사람들의 태도, 의견, 성향 분석



### 시각화

- Word Cloud
- Sentiment Pie Chart





# 단어의 표현

## One-Hot-Encoding

1. 단어를 숫자로 표현하는 가장 간단한 방법
   ex) 원숭이=[1,0,0]
2. 한계 : 단어의 수만큼 차원이 필요, 의미를 담지 못함(cos유사도 0)



## BoW(Bag of Word)

1. 문서 내 단어 출현 순서는 무시, 빈도수만 기반으로 문서를 표현
2. 벡터 공간 낭비, 연산비효율
3. 단어의 빈도 수가 중요도를 바로 의미 하지 않음
4. 전처리가 매우 중요함(같은의미의 다른단어를 인식 못함)



## TDM(Term-Document Matrix)

1. Bow 중 하나
2. 문서에 등장하는 각 단어 빈도를 행렬로 표현 한것
3. DTM과 transpose 관계
4. 단어의 빈도수가 중요도를 바로 의미 하지 않음(이를 보안하기위해 TFIDF사용)

## TF-IDF

1. 단어 빈도- 역문서 빈도

   | tf(d,t) | 특정 문서 d에서의 특정 단어 t의 등장 횟수 |
   | :---: | :------------------------------------- |
   |  **df(t)**  | **특정 단어 t 가 등장한 문서의 수** |
   | **idf(d, t)** | **df(t)의 역수** |




2. 단어와 문서 사이의 연관성
3. 검색어와 가장 관련이 있는 문서를 찾기, 키워드 추출 가능
4. 계산절차
```mermaid
graph LR
A(토큰 Index생성) --> B(__TF 계산__) --> C(__IDF계산__) --> D(TF-IDF계산)
```
- 각토큰을 그대로 사용 할수 없기 떄문에 토큰에 index를 부여
- 각 토큰의 등장 빈도수 계산(문서 내 토큰빈도/ 해당 문서의 전체 토큰 수)

- IDF 계산 `log(문서수/토큰이등장한 문서수)`
- TF-IDF계산 : TF와 IDF를 곱함

5. 값이 높은 순서대로 **핵심 키워드**를 뽑을 수있다



## Text Rank

1. 자카드 유사도를 이용해 문장간 유사도 구하기
2. Textrank 과정
3. 문장의 score 계산
4. 중요문장 선별

## Luhn Summarize

1. 단어의 사용빈도를 이용해 중요단어 결정한다(0.001<단어빈도비율<0.5)
2. 문장 중요도 계산(중요단어가 등장하는 처음과 끝사이 중요단어의 상대비율)
3. 문장중요도 순위별 출력

## Word Embedding

- 단어의 의미를 간직하는 밀집벡터로 표현하는 방법



## 유사도 계산

1. 유클리디언 거리(Euclidean distance)
2. 코사인 유사도
3. 자카트 유사도
4. 레벤슈타인 유사도



## n-gram

- 복수개의 단어를 봄(unigram, bigram, trigram 등)
- 제한적으로 문맥을 표현 할 수 있음



# 문서 분류

1. 나이브 베이즈 분류
2. 서포트벡터 머신 
3. 딥러닝



## Bayes Classifier

- 데이터의 조건부 확률에 기반한 분류-> 데이터 중심
- 범주형 자료에만 적용 가능 : 수치형 자료(키, 몸무게 등)

- 좋은 성능을 위해서는 대량데이터가 필요
- 종류 : Exact Bayes Classifier
            Naive Bayes Classifier

#### Naive Bayes Classifier

- 특성들 사이의 독립을 가정(실제로 독립이 아니지만 자료가많을수록 거의 유사함)
- 두 확률 변수의 사전 확률과 사후 확률 사이의 관계를 나타내는 정리

##### Laplace smmothing

- 입력 텍스트가 기존에 계산한 확률이 존재하지 않을 경우 0으로 계산 될 수 있음

- 분자 분모에 일정 상수 k를 더하여 신규 단어가 출현 했을 때 0으로 계산되는 것 방지








$$
P(w_i|positive) = \frac{k+count(w_i,positive)}{2k+\sum_{w\in d}}
$$

##### log 이용하여 언더플로우 방지

$$
\log A \cdot B = \log A + \log B
$$
> 확률을 계산하고 확률간 곱으로 연산이 이루어 짐
> 1 이하 값을 계속 곱하면 underflow 발생 -> log의 성질 활용



##### 성능개선

- 불용어처리
- 원형 복원
- N-gram
- TF-IDF



## 감정분석

##### 사전기반 감정분석

> - 사전기반 분석은 정의된 긍정,부정어 사전을 활용하여 일치 단어 등장 여부를 판단하여 측정 하는 방법
> - 사전의 질이 분석의 성능을 좌우함

###### 어려운점

> - 문맥에 따른 감정분석이 어려움
>   - ngram활용하여 문맥을 포함한 사전 생성
>   - Sequence를 처리 할 수 있는 딥러닝
> - 범용적 사전 적용이 어려움
>   - 도메인별 용어가 다름 -> 도메인별 사전 필요
> - 한글의 경우 감성 사전이 부족









## PCA

- 데이터 구조적 의미 : 각 feature의 변동이 얼마나 닮았나?
- 

## SVD(Singular Value Decomposition)

- 행렬은 선형변환이다

- 직교하는 벡터 집합에 대하여(차원과 동일한 수) 변환후에도 여전히 직교

- 특이값 분해는 임의의 $m\times n$ 차원의 행렬 A에 대하여 다음과 같이 행렬을 분해

- 고유값 분해가 정방 행렬에만 적용가능한데 비교하려 비정방행렬에도 적용이 가능
  $$
  A_k = U_k\Sigma_k{V_k}^T
  $$
  

## LSA

- 토픽을 추출된 키워드들의 분포로 나타냄으로써 텍스트 내의 구조를 파악



TDM만들기 ->

##### 한계점

- 다의어 문제 해결x ->LDA기법은 해결가능

## LDA(잠재디레클레 할당)

- 문서 내 토픽비율 : 문서내 각각의 토픽이 나타날 확률(다항분포)